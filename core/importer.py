import json
import psycopg2
import os
import logging
import re
from psycopg2.extras import execute_values
from config.settings import DB_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s')

class SDEImporter:
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            # ä½¿ç”¨æ¥è‡ª config.settings çš„ DB_CONFIG (ç¡®ä¿ host ä¸º 'db' æˆ– 'ruoyi-pg')
            self.conn = psycopg2.connect(**DB_CONFIG)
            self.conn.autocommit = False 
            logging.info("Successfully connected to PostgreSQL.")
        except Exception as e:
            logging.error(f"Database connection failed: {e}")
            raise

    def _camel_to_snake(self, name):
        """å°† CamelCase è½¬æ¢ä¸º snake_case"""
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    def auto_import(self, file_path, batch_size=1000):
        """
        è‡ªåŠ¨æ ¹æ®æ–‡ä»¶ååˆ›å»ºè¡¨å¹¶æ‰¹é‡å¯¼å…¥ JSONL æ•°æ®åˆ° raw æ¶æ„
        """
        base_name = os.path.basename(file_path).replace(".jsonl", "")
        
        # è½¬æ¢è¡¨åé€»è¾‘ï¼šé©¼å³°è½¬ä¸‹åˆ’çº¿ (ä¾‹å¦‚ invTypes -> inv_types)
        table_name = self._camel_to_snake(base_name)
        if base_name.startswith("_"):
            table_name = "_" + table_name

        cursor = self.conn.cursor()
        try:
            # 1. ç¡®ä¿æ¶æ„å­˜åœ¨å¹¶åˆ›å»ºè¡¨
            cursor.execute("CREATE SCHEMA IF NOT EXISTS raw;")
            cursor.execute(f"""
                CREATE TABLE IF NOT EXISTS raw.{table_name} (
                    id TEXT PRIMARY KEY,
                    data JSONB
                );
            """)

            logging.info(f"ğŸš€ å¼€å§‹å¯¼å…¥æ•°æ®åˆ° [raw.{table_name}]...")
            
            batch_data = []
            count = 0

            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    record = json.loads(line)
                    item_id = str(record.get("_key"))
                    
                    if item_id == "None":
                        continue
                    
                    # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®å…ƒç»„
                    batch_data.append((item_id, json.dumps(record)))
                    count += 1

                    # è¾¾åˆ°æ‰¹æ¬¡å¤§å°åæ‰§è¡Œæ’å…¥
                    if len(batch_data) >= batch_size:
                        self._execute_batch_upsert(cursor, table_name, batch_data)
                        batch_data = []
                        logging.info(f"  å·²å¤„ç† {count} æ¡è®°å½•...")

            # æ’å…¥å‰©ä½™çš„æ•°æ®
            if batch_data:
                self._execute_batch_upsert(cursor, table_name, batch_data)

            self.conn.commit()
            logging.info(f"âœ… [raw.{table_name}] å¯¼å…¥å®Œæˆï¼Œå…±è®¡ {count} æ¡è®°å½•ã€‚")

        except Exception as e:
            self.conn.rollback()
            logging.error(f"âŒ å¯¼å…¥ raw.{table_name} å¤±è´¥: {e}")
            raise 
        finally:
            cursor.close()

    def _execute_batch_upsert(self, cursor, table_name, data_list):
        """æ‰§è¡Œæ‰¹é‡ Upsert é€»è¾‘"""
        insert_sql = f"""
            INSERT INTO raw.{table_name} (id, data)
            VALUES %s
            ON CONFLICT (id) DO UPDATE SET data = EXCLUDED.data;
        """
        execute_values(cursor, insert_sql, data_list)

    def close(self):
        """æ˜¾å¼å…³é—­è¿æ¥"""
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
            logging.info("Database connection closed.")

    def __del__(self):
        self.close()